{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resource\n",
    "import sys\n",
    "import gc\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.data as ds\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from functools import partial\n",
    "import sklearn.cluster as clustering\n",
    "import sklearn.decomposition as decomposition\n",
    "\n",
    "print(tf.version.VERSION)\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-31T08:43:53.586582Z",
     "iopub.status.busy": "2021-07-31T08:43:53.586198Z",
     "iopub.status.idle": "2021-07-31T08:44:49.674376Z",
     "shell.execute_reply": "2021-07-31T08:44:49.673529Z",
     "shell.execute_reply.started": "2021-07-31T08:43:53.586490Z"
    }
   },
   "outputs": [],
   "source": [
    "# uncomment for debugging layers\n",
    "# tf.config.run_functions_eagerly(True) \n",
    "\n",
    "\n",
    "@tf.function\n",
    "def image_float_to_int(image): \n",
    "    return tf.image.convert_image_dtype(image, tf.uint8)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def load_image(file_path, img_size=[306, 306]):\n",
    "    raw = tf.io.read_file(file_path)\n",
    "    return tf.image.decode_jpeg(raw, channels=3)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def random_crop(image, size=128):\n",
    "    '''\n",
    "    crop a random box out of the image\n",
    "    :param image: image tensor to be modified\n",
    "    :param size: int. The pixel size of the cropped box\n",
    "    '''\n",
    "    s = tf.shape(image)\n",
    "    w = s[0]\n",
    "    h = s[1]\n",
    "    c = s[2]\n",
    "    dh = size\n",
    "    dw = size\n",
    "    dx = tf.random.uniform([1], minval=0, maxval=w-dw, dtype=tf.dtypes.int32, seed=None, name=None)[0]\n",
    "    dy = tf.random.uniform([1], minval=0, maxval=h-dh, dtype=tf.dtypes.int32, seed=None, name=None)[0]\n",
    "    offset_height = dy\n",
    "    target_height = dh\n",
    "    offset_width = dx\n",
    "    target_width = dw\n",
    "    image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, target_height, target_width)\n",
    "    return image\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def box_delete(image, size=32, l=0.9):\n",
    "    '''\n",
    "    :param image: image tensor to be modified\n",
    "    :param strength: int. The pixel size of the deleted box\n",
    "    '''\n",
    "    s = tf.shape(image)\n",
    "    w = s[0]\n",
    "    h = s[1]\n",
    "    c = s[2]\n",
    "    # compute size and position of mask\n",
    "    dh = size\n",
    "    dw = size\n",
    "    dx = tf.random.uniform([1], minval=0, maxval=w-dw, dtype=tf.dtypes.int32, seed=None, name=None)[0]\n",
    "    dy = tf.random.uniform([1], minval=0, maxval=h-dh, dtype=tf.dtypes.int32, seed=None, name=None)[0]\n",
    "    # prepare indices\n",
    "    xs = tf.range(start=dx, limit=dx+dw)\n",
    "    ys = tf.range(start=dy, limit=dy+dh)\n",
    "    X, Y = tf.meshgrid(ys, xs)\n",
    "    X = tf.reshape(X, [-1, 1])\n",
    "    Y = tf.reshape(Y, [-1, 1])\n",
    "    mask_indices = tf.concat([Y, X], axis=-1)\n",
    "    mask_indices = tf.reshape(mask_indices, [-1, 2])\n",
    "    # prepare image patch\n",
    "    updates_hard = tf.random.uniform([dw*dh, c], minval=0, maxval=1, dtype=tf.dtypes.float32) # hard noise\n",
    "    updates_soft = tf.random.uniform([dw*dh, c], minval=0, maxval=0.1, dtype=tf.dtypes.float32) # soft noise\n",
    "    updates_black = tf.zeros([dw*dh, c], dtype=tf.dtypes.float32) # black (zeros)\n",
    "    # overwrite image with patch\n",
    "    modified_image = tf.tensor_scatter_nd_update(image, mask_indices, updates_black, name=None) # replace with noise\n",
    "    lam = tf.constant(l, dtype=tf.dtypes.float32)\n",
    "    one_minus_lam = tf.constant(1-l, dtype=tf.dtypes.float32)\n",
    "    updates_weights = tf.ones([dw*dh, c], dtype=tf.dtypes.float32) * one_minus_lam\n",
    "    weights = tf.tensor_scatter_nd_update(tf.ones_like(image, dtype=tf.dtypes.float32) * lam, mask_indices, updates_weights, name=None)\n",
    "    weights = tf.reshape(weights, [-1,])\n",
    "#     modified_image = tf.tensor_scatter_nd_add(image, mask_indices, updates_black, name=None) # add noise\n",
    "    return modified_image, image, weights\n",
    "\n",
    "@tf.function\n",
    "def add_noise(image, strength=0.1):\n",
    "    '''\n",
    "    :param image: image tensor to be modified\n",
    "    :param strength: float, [0,1]. The amount of noise to add\n",
    "    '''\n",
    "    s = tf.shape(image)\n",
    "    w = s[0]\n",
    "    h = s[1]\n",
    "    c = s[2]\n",
    "    # prepare noise\n",
    "    noise = tf.random.uniform([w, h, c], minval=0, maxval=strength, dtype=tf.dtypes.float32) # soft noise\n",
    "    # add noise\n",
    "    modified_image = image + noise\n",
    "    weights = tf.ones_like(image, dtype=tf.dtypes.float32)\n",
    "    weights = tf.reshape(weights, [-1,])\n",
    "    return modified_image, image, weights\n",
    "\n",
    "def flatten_labels(modified_image, image, weights):\n",
    "    flattened = tf.reshape(image, [-1,1])\n",
    "    return modified_image, flattened, weights\n",
    "\n",
    "## visualize the dataset\n",
    "def visualize_dst(dst):\n",
    "    print_images = dst.take(9)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, images, weights in enumerate(print_images):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images.numpy().astype(\"uint8\"))\n",
    "        #plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "    return dst\n",
    "\n",
    "\n",
    "def visualize_training(dst):\n",
    "    dst = dst.take(3)\n",
    "    plt.figure(figsize=(6, 10))\n",
    "    i = 0\n",
    "    for image, gt, weights in dst:\n",
    "        image = tf.image.convert_image_dtype(image, tf.uint8)\n",
    "        ax = plt.subplot(3, 2, i + 1)\n",
    "        plt.imshow(image.numpy().astype(\"uint8\"))\n",
    "        i+=1\n",
    "        plt.axis(\"off\")\n",
    "        gt = tf.image.convert_image_dtype(gt, tf.uint8)\n",
    "        ax = plt.subplot(3, 2, i + 1)\n",
    "        plt.imshow(gt.numpy().astype(\"uint8\"))\n",
    "        i+=1\n",
    "        plt.axis(\"off\")\n",
    "        #plt.title(class_names[labels[i]])\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an input pipeline and we can see and verify our data, lets create a model.\n",
    "Given image A and its distorted version D(A), compute a recovered version R(D(A)) which is as close as possible to the original A:\n",
    "argmin_W{|A-R_W(D(A))|^2}\n",
    "\n",
    "We will go with a unet model which will encode the features to each pixel. We will then train the DNN on the above task.\n",
    "Thus for an input x the model will output:\n",
    "f(x)=En(x) - the pixel wise features\n",
    "R(x)=De(x) - the recovered picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T12:43:41.741342Z",
     "iopub.status.busy": "2021-07-30T12:43:41.74099Z",
     "iopub.status.idle": "2021-07-30T12:43:41.747058Z",
     "shell.execute_reply": "2021-07-30T12:43:41.746162Z",
     "shell.execute_reply.started": "2021-07-30T12:43:41.741303Z"
    }
   },
   "outputs": [],
   "source": [
    "# inspect graphs to find if memory leak is here\n",
    "tf.executing_eagerly()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T12:43:41.749917Z",
     "iopub.status.busy": "2021-07-30T12:43:41.749377Z",
     "iopub.status.idle": "2021-07-30T12:43:41.807513Z",
     "shell.execute_reply": "2021-07-30T12:43:41.806581Z",
     "shell.execute_reply.started": "2021-07-30T12:43:41.749865Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvBlock(keras.layers.Layer):\n",
    "    '''\n",
    "    convolution block for unet with optional max pooling\n",
    "    '''\n",
    "    def __init__(self, filters, downsample=True, bname='', **kwargs):\n",
    "        self.filters = filters\n",
    "        self.bname = bname\n",
    "        self.downsample = downsample\n",
    "        super(ConvBlock, self).__init__(**kwargs)\n",
    "        self.conv1 = keras.layers.Conv2D(filters=self.filters, kernel_size=(3,3), strides=(1, 1),\n",
    "                            padding=\"same\", data_format='channels_last', kernel_initializer=\"glorot_uniform\",\n",
    "                            bias_initializer=\"zeros\", name=bname+\"_conv_1\")\n",
    "        self.conv2 = keras.layers.Conv2D(filters=self.filters, kernel_size=(3,3), strides=(1, 1),\n",
    "                            padding=\"same\", data_format='channels_last', kernel_initializer=\"glorot_uniform\",\n",
    "                            bias_initializer=\"zeros\", name=bname+\"_conv_2\")\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'filters': self.filters,\n",
    "            'bname': self.bname,\n",
    "            'downsample': self.downsample\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = keras.layers.ReLU(name=self.bname+\"_relu_1\")(x)\n",
    "        # stride 2, padding 1, k=3, o=i/2\n",
    "#         x = keras.layers.ZeroPadding2D(padding=(1, 1), data_format='channels_last', name=self.bname+\"_zeroPad\")(x) \n",
    "        x = self.conv2(x)\n",
    "        x = keras.layers.ReLU(name=self.bname+\"_relu_2\")(x)\n",
    "        p = keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid', \n",
    "                                      name=self.bname+\"_maxpool\")(x)\n",
    "        return x, p\n",
    "\n",
    "\n",
    "class DeConvBlock(keras.layers.Layer):\n",
    "    '''\n",
    "    convolution block for unet with optional max pooling\n",
    "    '''\n",
    "    def __init__(self, filters, bname='', **kwargs):\n",
    "        self.filters = filters\n",
    "        self.bname = bname\n",
    "        super(DeConvBlock, self).__init__(**kwargs)\n",
    "        self.conv1 = keras.layers.Conv2DTranspose(filters=self.filters, kernel_size=(3,3), strides=2,\n",
    "                            padding=\"same\", data_format='channels_last', kernel_initializer=\"glorot_uniform\",\n",
    "                            bias_initializer=\"zeros\", name=bname+\"_deconv\")\n",
    "        self.conv2 = keras.layers.Conv2D(filters=self.filters, kernel_size=(3,3), strides=1,\n",
    "                            padding=\"same\", data_format='channels_last', kernel_initializer=\"glorot_uniform\",\n",
    "                            bias_initializer=\"zeros\", name=bname+\"_conv\")\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'filters': self.filters,\n",
    "            'bname': self.bname\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = keras.layers.ReLU(name=self.bname+\"_relu_1\")(x)\n",
    "        x = self.conv2(x)\n",
    "        x = keras.layers.ReLU(name=self.bname+\"_relu_2\")(x)\n",
    "        return x\n",
    "\n",
    "class PaddedBase(keras.layers.Layer):\n",
    "    '''\n",
    "    base class for layers involving padding one input to match the other\n",
    "    '''\n",
    "    def build(self, input_shape):\n",
    "        self.shape1 = input_shape[0]\n",
    "        self.shape2 = input_shape[1]\n",
    "        dh = self.shape1[1] - self.shape2[1]\n",
    "        dw = self.shape1[2] - self.shape2[2]\n",
    "        self.bigger_shape = self.shape1[:3]\n",
    "        self.smaller_shape = self.shape2[:3]\n",
    "        self.c1 = self.shape1[3]\n",
    "        self.c2 = self.shape2[3]\n",
    "        self.pred = tf.logical_or(tf.math.less(dw, 0), tf.math.less(dh, 0))\n",
    "        if self.pred:\n",
    "            dh = - dh\n",
    "            dw = - dw\n",
    "            self.bigger_shape = self.shape2[:3]\n",
    "            self.smaller_shape = self.shape1[:3]\n",
    "        hmod = tf.math.floormod(dh, 2)\n",
    "        hdiv = tf.cast(tf.math.divide(dh - hmod, 2), tf.int32)\n",
    "        wmod = tf.math.floormod(dw, 2)\n",
    "        wdiv = tf.cast(tf.math.divide(dw - wmod, 2), tf.int32)\n",
    "        dhup = hdiv\n",
    "        dhdown = hdiv + hmod\n",
    "        dwleft = wdiv\n",
    "        dwright = wdiv + wmod\n",
    "        padB = tf.constant([0, 0])\n",
    "        padh = tf.stack([dhup, dhdown], axis=-1)\n",
    "        padw = tf.stack([dwleft, dwright], axis=-1)\n",
    "        padC = tf.constant([0, 0])\n",
    "        self.paddings = tf.stack([padB, padh, padw, padC], axis=0)\n",
    "\n",
    "class PadToSize(PaddedBase):\n",
    "    '''\n",
    "    padding the smaller tensor so the shapes match and return the first\n",
    "    '''\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        tensor_1, tensor_2 = inputs\n",
    "        tensor_3 =  tf.pad(tensor_1, paddings=self.paddings , mode='CONSTANT', constant_values=0)\n",
    "        return tensor_3\n",
    "        \n",
    "class PaddedConcat(PaddedBase):\n",
    "    '''\n",
    "    concats two tensors, padding the smaller tensor so the shapes match\n",
    "    ''' \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        tensor_1, tensor_2 = inputs\n",
    "        tensor_3 =  tf.pad(tensor_1, paddings=self.paddings , mode='CONSTANT', constant_values=0)\n",
    "        out = tf.concat([tensor_3, tensor_2], axis=-1)\n",
    "        return out\n",
    "\n",
    "class PaddedAdd(PaddedBase):\n",
    "    '''\n",
    "    concats two tensors, padding the smaller tensor so the shapes match\n",
    "    '''\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        tensor_1, tensor_2 = inputs\n",
    "        tensor_3 =  tf.pad(tensor_1, paddings=self.paddings , mode='CONSTANT', constant_values=0)\n",
    "        out = tf.math.add(tensor_3, tensor_2)\n",
    "        return out\n",
    "    \n",
    "class CropToShape(keras.layers.Layer):\n",
    "    '''\n",
    "    crop input to specified shape\n",
    "    '''\n",
    "    def build(self, input_shape):\n",
    "        shape1 = input_shape[0]\n",
    "        shape2 = input_shape[1]\n",
    "        self.h = shape2[1]\n",
    "        self.w = shape2[2]\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        return tf.slice(inputs[0], [0, 0, 0, 0], [-1, self.h, self.w, -1])\n",
    "\n",
    "    \n",
    "def bridge(x, bridge_features, num=0):\n",
    "    if len(bridge_features) > 0:\n",
    "        name = \"bridge_layer_\"+str(num)\n",
    "        x = keras.layers.Conv2D(filters=bridge_features[0], kernel_size=(3,3), strides=(1, 1),\n",
    "                            padding=\"same\", data_format='channels_last', kernel_initializer=\"glorot_uniform\",\n",
    "                            bias_initializer=\"zeros\", name=name+\"_conv\")(x)\n",
    "        x = keras.layers.ReLU(name=name+\"_relu\")(x)\n",
    "        return bridge(x, bridge_features[1:], num+1)\n",
    "    else:\n",
    "        return x\n",
    "        \n",
    "    \n",
    "def Unet(x, bridge_features, encoder_filters_list, decoder_filters_list, skip_connections, level=0):\n",
    "    tf.debugging.assert_shapes([(encoder_filters_list,('N')),(decoder_filters_list,('N'))],\n",
    "                               message=\"encoder and decoder have different length!\")\n",
    "    #encoder\n",
    "    bname = \"encoder_layer_\"+str(level)\n",
    "    xout, p = ConvBlock(encoder_filters_list[0], bname=bname)(x)\n",
    "    if len(encoder_filters_list) > 1:\n",
    "        x = Unet(p, bridge_features, encoder_filters_list[1:], \n",
    "                 decoder_filters_list[:-1], skip_connections[:-1], level=level+1)\n",
    "    else:\n",
    "        x = bridge(p, bridge_features, num=0)\n",
    "    #decoder\n",
    "    bname = \"decoder_features_\"+str(level)\n",
    "    x = DeConvBlock(decoder_filters_list[-1], bname=bname)(x)\n",
    "    if skip_connections[-1]==1:\n",
    "        # skip connection\n",
    "        print(\"x:\", x)\n",
    "        print(\"xout:\", xout)\n",
    "        x = PaddedConcat(name=bname+\"_concat\")([x, xout])\n",
    "#         x = PaddedAdd(name=bname+\"_add\")([x, xout])\n",
    "    else:\n",
    "        # no skip connection here so just padd as necessary\n",
    "        x = PadToSize()([x, xout])\n",
    "    return x\n",
    "    \n",
    "\n",
    "def AutoEncoder(x, bridge_features, encoder_filters_list, decoder_filters_list, level=0):\n",
    "    tf.debugging.assert_shapes([(encoder_filters_list,('N')),(decoder_filters_list,('N'))],\n",
    "                               message=\"encoder and decoder have different length!\")\n",
    "    #encoder\n",
    "    bname = \"encoder_layer_\"+str(level)\n",
    "    xout, p = ConvBlock(encoder_filters_list[0], bname=bname)(x)\n",
    "    if len(encoder_filters_list) > 1:\n",
    "        x = AutoEncoder(p, bridge_features, encoder_filters_list[1:], decoder_filters_list[:-1], level=level+1)\n",
    "    else:\n",
    "        x = bridge(p, bridge_features, num=0)\n",
    "    #decoder\n",
    "    bname = \"decoder_features_\"+str(level)\n",
    "    x = DeConvBlock(decoder_filters_list[-1], bname=bname)(x)  \n",
    "    x = PadToSize()([x, xout])\n",
    "    return x\n",
    "        \n",
    "\n",
    "def get_completion_model(bridge_features, encoder_filters_list, decoder_filters_list, skip_connections,\n",
    "                         head_filters_list):\n",
    "    image = keras.layers.Input(shape=[128,128, 3], dtype='float32', name=\"input_org\")\n",
    "    features = Unet(image, bridge_features, encoder_filters_list, decoder_filters_list, skip_connections)\n",
    "#     features = AutoEncoder(image, bridge_features, encoder_filters_list, decoder_filters_list)\n",
    "    kernel_size = (1,1) #(3,3)\n",
    "    for i, f in enumerate(head_filters_list):\n",
    "        features_in = features\n",
    "        features = keras.layers.Conv2D(filters=f, kernel_size=kernel_size, strides=(1, 1),\n",
    "                            padding=\"same\", data_format='channels_last', kernel_initializer=\"glorot_uniform\",\n",
    "                            bias_initializer=\"zeros\")(features)\n",
    "        features = keras.layers.ReLU(name=\"head_conv_\"+str(i)+\"_conv_out\")(features)\n",
    "    recovered_image = features\n",
    "    flattened_recovered_image = tf.keras.layers.Reshape([-1,1])(recovered_image)\n",
    "    return keras.Model(inputs=image, outputs=flattened_recovered_image)\n",
    "\n",
    "def get_simple_model():\n",
    "    image = keras.layers.Input(shape=[128,128, 3], dtype='float32', name=\"input_org\")\n",
    "    features = keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=(1, 1), padding=\"same\", data_format='channels_last')(image)\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding=\"valid\", data_format='channels_last')\n",
    "    features = keras.layers.Conv2D(filters=8, kernel_size=(3,3), strides=(1, 1), padding=\"same\", data_format='channels_last')(features)\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding=\"valid\", data_format='channels_last')\n",
    "    features = keras.layers.Conv2D(filters=8, kernel_size=(3,3), strides=(1, 1), padding=\"same\", data_format='channels_last')(features)\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding=\"valid\", data_format='channels_last')\n",
    "    features = keras.layers.Conv2D(filters=8, kernel_size=(3,3), strides=(1, 1), padding=\"same\", data_format='channels_last')(features)\n",
    "    tf.keras.layers.UpSampling2D(size=(2, 2), data_format='channels_last', interpolation=\"nearest\")\n",
    "    features = keras.layers.Conv2D(filters=8, kernel_size=(3,3), strides=(1, 1), padding=\"same\", data_format='channels_last')(features)\n",
    "    tf.keras.layers.UpSampling2D(size=(2, 2), data_format='channels_last', interpolation=\"nearest\")\n",
    "    features = keras.layers.Conv2D(filters=8, kernel_size=(3,3), strides=(1, 1), padding=\"same\", data_format='channels_last')(features)\n",
    "    tf.keras.layers.UpSampling2D(size=(2, 2), data_format='channels_last', interpolation=\"nearest\")\n",
    "    features = keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=(1, 1), padding=\"same\", data_format='channels_last')(features)\n",
    "    features = keras.layers.Conv2D(filters=3, kernel_size=(3,3), strides=(1, 1), padding=\"same\", data_format='channels_last')(features)\n",
    "    features = tf.keras.layers.Reshape([-1,1])(features)\n",
    "    return keras.Model(inputs=image, outputs=features)\n",
    "\n",
    "class PixelWiseHuberVisualize(keras.losses.Loss):\n",
    "    def call(self, img_true, img_pred):\n",
    "        #img_pred = tf.reshape(tf.convert_to_tensor_v2(img_pred), [-1, 1])\n",
    "#         img_pred = tf.reshape(img_pred, [-1, 1])\n",
    "#         img_true = tf.reshape(tf.cast(img_true, img_pred.dtype), [-1, 1])\n",
    "        # image values are in [0,1] so put delta in the middle\n",
    "        h = tf.math.squared_difference(img_pred, img_true)\n",
    "#         h = tf.keras.losses.Huber(delta=0.5, reduction=keras.losses.Reduction.NONE)\n",
    "#         return h(img_pred, img_true)\n",
    "        return h\n",
    "\n",
    "\n",
    "class PixelWiseHuber(keras.losses.Loss):\n",
    "    @tf.function\n",
    "    def call(self, img_true, img_pred):\n",
    "        #img_pred = tf.reshape(tf.convert_to_tensor_v2(img_pred), [-1, 1])\n",
    "#         img_pred = tf.reshape(img_pred, [-1, 1])\n",
    "#         img_true = tf.reshape(tf.cast(img_true, img_pred.dtype), [-1, 1])\n",
    "        # image values are in [0,1] so put delta in the middle\n",
    "        shape1 = tf.shape(img_pred)\n",
    "        shape2 = tf.shape(img_true)\n",
    "#         tf.print(\"pred shape=\", shape1, \"original shape=\", shape2)\n",
    "        return tf.keras.losses.Huber(delta=0.5)(img_pred, img_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the loss function to see that it gives correct values. Test by adding noise to an image and plotting the loss as a function of SNR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T12:43:41.809601Z",
     "iopub.status.busy": "2021-07-30T12:43:41.809245Z",
     "iopub.status.idle": "2021-07-30T12:43:41.833919Z",
     "shell.execute_reply": "2021-07-30T12:43:41.83301Z",
     "shell.execute_reply.started": "2021-07-30T12:43:41.809564Z"
    }
   },
   "outputs": [],
   "source": [
    "# inspect graphs to find if memory leak is here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T12:43:41.836297Z",
     "iopub.status.busy": "2021-07-30T12:43:41.835463Z",
     "iopub.status.idle": "2021-07-30T12:43:42.355084Z",
     "shell.execute_reply": "2021-07-30T12:43:42.354226Z",
     "shell.execute_reply.started": "2021-07-30T12:43:41.836259Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def add_gaussian_noise(image, invSNR):\n",
    "    A = tf.norm(image)\n",
    "    std = invSNR * A\n",
    "    noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=std)\n",
    "    return tf.math.add(image, noise)\n",
    "\n",
    "\n",
    "def visualize_loss(dst):\n",
    "    original = dst.take(1)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for original in original:\n",
    "        aug, org, w = original\n",
    "        #     loss_image = original.map(tf.math.squared_difference)\n",
    "        org = tf.reshape(org, tf.shape(aug))\n",
    "        loss_image = keras.losses.huber(aug, org)\n",
    "        loss_image = tf.image.convert_image_dtype(loss_image, tf.uint8)\n",
    "        aug = tf.image.convert_image_dtype(aug, tf.uint8)\n",
    "        org = tf.image.convert_image_dtype(org, tf.uint8)\n",
    "        ax = plt.subplot(1,3,1)\n",
    "        plt.imshow(loss_image.numpy().astype(\"uint8\"))\n",
    "        ax = plt.subplot(1,3,2)\n",
    "        plt.imshow(aug.numpy().astype(\"uint8\"))\n",
    "        ax = plt.subplot(1,3,3)\n",
    "        plt.imshow(org.numpy().astype(\"uint8\"))\n",
    "    return dst\n",
    "\n",
    "class GCCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()\n",
    "\n",
    "class MemoryCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, logfile):\n",
    "        super(MemoryCallback, self).__init__()\n",
    "        self.logfile = logfile\n",
    "        if os.path.exists(logfile):\n",
    "            os.remove(logfile)\n",
    "    def on_train_batch_begin(self, batch, logs={}):\n",
    "        if batch % 100 == 0:\n",
    "            print(' ',resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n",
    "            with open(self.logfile,'a') as f:\n",
    "                f.writelines([f'{batch} {resource.getrusage(resource.RUSAGE_SELF).ru_maxrss}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets train. params:\n",
    "- unet_num_features\n",
    "- unet_filters_list\n",
    "- head_filters_list\n",
    "Note that the head capabilities strongly depend on how many filters are applied: The maximum \"interpolation length\" is ~ 3*num_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T12:43:42.356576Z",
     "iopub.status.busy": "2021-07-30T12:43:42.35624Z"
    }
   },
   "outputs": [],
   "source": [
    "# station = \"home\"\n",
    "station = \"aws\"\n",
    "train=False\n",
    "evaluate=False\n",
    "\n",
    "if station==\"aws\":\n",
    "    home = os.getenv(\"HOME\")\n",
    "elif station==\"home\":\n",
    "    home = \"/home/lior/PycharmProjects/facesTasks\"\n",
    "    \n",
    "print(\"current working directory:\", home)\n",
    "\n",
    "copies_per_image = 1\n",
    "images_per_batch = 3\n",
    "dataset = tf.data.Dataset.list_files(f'{home}/images/*.jpg')\n",
    "# train/test split\n",
    "image_count = tf.cast(dataset.cardinality(), tf.float32)\n",
    "train_perc = tf.constant(0.8)\n",
    "train_dataset = dataset.take(tf.cast(tf.math.round(image_count * train_perc), tf.int64))\n",
    "val_dataset = dataset.skip(tf.cast(tf.math.round(image_count * train_perc), tf.int64))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000)\n",
    "val_dataset = val_dataset.shuffle(buffer_size=1000)\n",
    "# train several times on each image (augmentations will be different)\n",
    "# train_dataset = train_dataset.interleave(lambda x: tf.data.Dataset.from_tensors(x)#.repeat(copies_per_image)\n",
    "#                                          ,cycle_length=4, block_length=copies_per_image)\n",
    "# val_dataset = val_dataset.interleave(lambda x: tf.data.Dataset.from_tensors(x)#.repeat(copies_per_image)\n",
    "#                                      ,cycle_length=4, block_length=copies_per_image)\n",
    "# read the images\n",
    "train_dataset = train_dataset.map(load_image, num_parallel_calls=3).cache()\n",
    "train_dataset = train_dataset.map(partial(tf.image.convert_image_dtype, dtype=tf.float32), num_parallel_calls=3)\n",
    "val_dataset = val_dataset.map(load_image, num_parallel_calls=3).cache()\n",
    "val_dataset = val_dataset.map(partial(tf.image.convert_image_dtype, dtype=tf.float32), num_parallel_calls=3)\n",
    "\n",
    "# prepare augmented images. dataset of (augmented image, image), block_length augmentions for each\n",
    "train_dataset = train_dataset.map(partial(random_crop, size=128), num_parallel_calls=3)\n",
    "train_dataset = train_dataset.map(partial(box_delete, size=32, l=0.06), num_parallel_calls=3)\n",
    "\n",
    "val_dataset = val_dataset.map(partial(random_crop, size=128), num_parallel_calls=3)\n",
    "val_dataset = val_dataset.map(partial(box_delete, size=32, l=0.06), num_parallel_calls=3)\n",
    "\n",
    "# visualize\n",
    "train_dataset.apply(visualize_training)\n",
    "\n",
    "# save current state of pipeline for segmentation ahead\n",
    "train_dataset_seg = train_dataset\n",
    "val_dataset_seg = val_dataset\n",
    "\n",
    "\n",
    "# flatten the original image (\"label\") so we can use sample weights\n",
    "train_dataset = train_dataset.map(partial(flatten_labels), num_parallel_calls=3)\n",
    "val_dataset = val_dataset.map(partial(flatten_labels), num_parallel_calls=3)\n",
    "# shuffle and batch\n",
    "train_dataset = train_dataset.batch(images_per_batch * copies_per_image, drop_remainder=True)\n",
    "val_dataset   = val_dataset.batch(images_per_batch * copies_per_image, drop_remainder=True)\n",
    "# prefetch\n",
    "train_dataset = train_dataset.prefetch(10)\n",
    "val_dataset = val_dataset.prefetch(4)\n",
    "\n",
    "print(\"done dataset preparations\")\n",
    "print(train_dataset)\n",
    "\n",
    "# AE V1\n",
    "#bridge_features = [16]\n",
    "# encoder_filters_list = [ 64, 64, 128, 128, 256, 256]\n",
    "# decoder_filters_list = [256, 256, 128, 128, 64, 64]\n",
    "# head_filters_list = [32, 32, 16, 3]\n",
    "bridge_features = [16, 16, 16, 16]\n",
    "encoder_filters_list = [128, 64, 32, 32, 32]\n",
    "skip_connections =     [1 ,  1 , 1,  0,  0]\n",
    "decoder_filters_list = [32, 32, 32,  64, 128]\n",
    "head_filters_list = [3]\n",
    "\n",
    "cp_file = f'{home}/session/model_cp.h5'\n",
    "if os.path.exists(cp_file):\n",
    "    print(\"found checkpoint, loading\")\n",
    "    model = tf.keras.models.load_model(cp_file, \n",
    "                               custom_objects={'ConvBlock': ConvBlock, 'DeConvBlock': DeConvBlock, \n",
    "                                               'PadToSize': PadToSize, 'PixelWiseHuber': PixelWiseHuber,\n",
    "                                               'PaddedConcat': PaddedConcat, 'PaddedAdd': PaddedAdd})\n",
    "    # test model\n",
    "    if evaluate:\n",
    "        print(\"evaluating model on val_dataset:\")\n",
    "        results = model.evaluate(val_dataset)\n",
    "        print(dict(zip(model.metrics_names, results)))\n",
    "\n",
    "    \n",
    "else:\n",
    "    print(\"checkpoint not found, compiling fresh model\")\n",
    "    model = get_completion_model(bridge_features, encoder_filters_list, decoder_filters_list, skip_connections,\n",
    "                                 head_filters_list)\n",
    "#     model = get_simple_model() # for debugging memory leak and comparison with our model\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001, epsilon=0.001) # default lr, use reduction strategy below\n",
    "    model.compile(optimizer=opt, loss=tf.keras.losses.MeanSquaredError(), metrics=['accuracy'])\n",
    "\n",
    "# garbage_collect_cb = GCCallback()\n",
    "# lr_strategy_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\", factor=0.1, patience=3, verbose=1,\n",
    "#                                                                 mode=\"min\", min_delta=0.001, cooldown=0, min_lr=0.0000001)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if not station==\"home\" and train==True: # don't train on home pc\n",
    "    history = model.fit(x=train_dataset, #steps_per_epoch=2000, \n",
    "                    epochs=100, validation_data=val_dataset, validation_steps=5, validation_freq=1,\n",
    "                        workers=3\n",
    "                        , callbacks=[tf.keras.callbacks.ModelCheckpoint(cp_file, verbose=1)]\n",
    "                   )\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.savefig(f'{home}/train_history.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_val = val_dataset.unbatch().shuffle(buffer_size=100).shard(num_shards=100, index=0)\n",
    "\n",
    "def visualize_results(dst):\n",
    "    dst = dst.take(6)\n",
    "    plt.figure(figsize=(10, 20))\n",
    "    i = 0\n",
    "    for image, gt, weights, result in dst:\n",
    "        image = tf.image.convert_image_dtype(image, tf.uint8)\n",
    "        ax = plt.subplot(6, 3, i + 1)\n",
    "        plt.imshow(image.numpy().astype(\"uint8\"))\n",
    "        i+=1\n",
    "        plt.axis(\"off\")\n",
    "        gt = tf.image.convert_image_dtype(gt, tf.uint8)\n",
    "        ax = plt.subplot(6, 3, i + 1)\n",
    "        plt.imshow(gt.numpy().astype(\"uint8\"))\n",
    "        i+=1\n",
    "        plt.axis(\"off\")\n",
    "        result = tf.image.convert_image_dtype(result, tf.uint8)\n",
    "        ax = plt.subplot(6, 3, i + 1)\n",
    "        plt.imshow(result.numpy().astype(\"uint8\"))\n",
    "        i+=1\n",
    "        plt.axis(\"off\")\n",
    "    return dst\n",
    "\n",
    "def do_model(image, gt, w, model):\n",
    "    shape = tf.shape(image)\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    result = model(image)\n",
    "    image = tf.reshape(image, shape)\n",
    "    result = tf.reshape(result, shape)\n",
    "    gt = tf.reshape(gt, shape)\n",
    "    return image, gt, w, result\n",
    "do_my_model = lambda x, y, z: do_model(x, y, z, model)\n",
    "results_val = vis_val.map(do_my_model)\n",
    "print(results_val)\n",
    "results_val.apply(visualize_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this model as a segmentation model by removing the head and using a clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google LLC.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# Author: Anton Mikhailov\n",
    "\n",
    "turbo_colormap_data = np.array([[0.18995,0.07176,0.23217],[0.19483,0.08339,0.26149],[0.19956,0.09498,0.29024],[0.20415,0.10652,0.31844],[0.20860,0.11802,0.34607],[0.21291,0.12947,0.37314],[0.21708,0.14087,0.39964],[0.22111,0.15223,0.42558],[0.22500,0.16354,0.45096],[0.22875,0.17481,0.47578],[0.23236,0.18603,0.50004],[0.23582,0.19720,0.52373],[0.23915,0.20833,0.54686],[0.24234,0.21941,0.56942],[0.24539,0.23044,0.59142],[0.24830,0.24143,0.61286],[0.25107,0.25237,0.63374],[0.25369,0.26327,0.65406],[0.25618,0.27412,0.67381],[0.25853,0.28492,0.69300],[0.26074,0.29568,0.71162],[0.26280,0.30639,0.72968],[0.26473,0.31706,0.74718],[0.26652,0.32768,0.76412],[0.26816,0.33825,0.78050],[0.26967,0.34878,0.79631],[0.27103,0.35926,0.81156],[0.27226,0.36970,0.82624],[0.27334,0.38008,0.84037],[0.27429,0.39043,0.85393],[0.27509,0.40072,0.86692],[0.27576,0.41097,0.87936],[0.27628,0.42118,0.89123],[0.27667,0.43134,0.90254],[0.27691,0.44145,0.91328],[0.27701,0.45152,0.92347],[0.27698,0.46153,0.93309],[0.27680,0.47151,0.94214],[0.27648,0.48144,0.95064],[0.27603,0.49132,0.95857],[0.27543,0.50115,0.96594],[0.27469,0.51094,0.97275],[0.27381,0.52069,0.97899],[0.27273,0.53040,0.98461],[0.27106,0.54015,0.98930],[0.26878,0.54995,0.99303],[0.26592,0.55979,0.99583],[0.26252,0.56967,0.99773],[0.25862,0.57958,0.99876],[0.25425,0.58950,0.99896],[0.24946,0.59943,0.99835],[0.24427,0.60937,0.99697],[0.23874,0.61931,0.99485],[0.23288,0.62923,0.99202],[0.22676,0.63913,0.98851],[0.22039,0.64901,0.98436],[0.21382,0.65886,0.97959],[0.20708,0.66866,0.97423],[0.20021,0.67842,0.96833],[0.19326,0.68812,0.96190],[0.18625,0.69775,0.95498],[0.17923,0.70732,0.94761],[0.17223,0.71680,0.93981],[0.16529,0.72620,0.93161],[0.15844,0.73551,0.92305],[0.15173,0.74472,0.91416],[0.14519,0.75381,0.90496],[0.13886,0.76279,0.89550],[0.13278,0.77165,0.88580],[0.12698,0.78037,0.87590],[0.12151,0.78896,0.86581],[0.11639,0.79740,0.85559],[0.11167,0.80569,0.84525],[0.10738,0.81381,0.83484],[0.10357,0.82177,0.82437],[0.10026,0.82955,0.81389],[0.09750,0.83714,0.80342],[0.09532,0.84455,0.79299],[0.09377,0.85175,0.78264],[0.09287,0.85875,0.77240],[0.09267,0.86554,0.76230],[0.09320,0.87211,0.75237],[0.09451,0.87844,0.74265],[0.09662,0.88454,0.73316],[0.09958,0.89040,0.72393],[0.10342,0.89600,0.71500],[0.10815,0.90142,0.70599],[0.11374,0.90673,0.69651],[0.12014,0.91193,0.68660],[0.12733,0.91701,0.67627],[0.13526,0.92197,0.66556],[0.14391,0.92680,0.65448],[0.15323,0.93151,0.64308],[0.16319,0.93609,0.63137],[0.17377,0.94053,0.61938],[0.18491,0.94484,0.60713],[0.19659,0.94901,0.59466],[0.20877,0.95304,0.58199],[0.22142,0.95692,0.56914],[0.23449,0.96065,0.55614],[0.24797,0.96423,0.54303],[0.26180,0.96765,0.52981],[0.27597,0.97092,0.51653],[0.29042,0.97403,0.50321],[0.30513,0.97697,0.48987],[0.32006,0.97974,0.47654],[0.33517,0.98234,0.46325],[0.35043,0.98477,0.45002],[0.36581,0.98702,0.43688],[0.38127,0.98909,0.42386],[0.39678,0.99098,0.41098],[0.41229,0.99268,0.39826],[0.42778,0.99419,0.38575],[0.44321,0.99551,0.37345],[0.45854,0.99663,0.36140],[0.47375,0.99755,0.34963],[0.48879,0.99828,0.33816],[0.50362,0.99879,0.32701],[0.51822,0.99910,0.31622],[0.53255,0.99919,0.30581],[0.54658,0.99907,0.29581],[0.56026,0.99873,0.28623],[0.57357,0.99817,0.27712],[0.58646,0.99739,0.26849],[0.59891,0.99638,0.26038],[0.61088,0.99514,0.25280],[0.62233,0.99366,0.24579],[0.63323,0.99195,0.23937],[0.64362,0.98999,0.23356],[0.65394,0.98775,0.22835],[0.66428,0.98524,0.22370],[0.67462,0.98246,0.21960],[0.68494,0.97941,0.21602],[0.69525,0.97610,0.21294],[0.70553,0.97255,0.21032],[0.71577,0.96875,0.20815],[0.72596,0.96470,0.20640],[0.73610,0.96043,0.20504],[0.74617,0.95593,0.20406],[0.75617,0.95121,0.20343],[0.76608,0.94627,0.20311],[0.77591,0.94113,0.20310],[0.78563,0.93579,0.20336],[0.79524,0.93025,0.20386],[0.80473,0.92452,0.20459],[0.81410,0.91861,0.20552],[0.82333,0.91253,0.20663],[0.83241,0.90627,0.20788],[0.84133,0.89986,0.20926],[0.85010,0.89328,0.21074],[0.85868,0.88655,0.21230],[0.86709,0.87968,0.21391],[0.87530,0.87267,0.21555],[0.88331,0.86553,0.21719],[0.89112,0.85826,0.21880],[0.89870,0.85087,0.22038],[0.90605,0.84337,0.22188],[0.91317,0.83576,0.22328],[0.92004,0.82806,0.22456],[0.92666,0.82025,0.22570],[0.93301,0.81236,0.22667],[0.93909,0.80439,0.22744],[0.94489,0.79634,0.22800],[0.95039,0.78823,0.22831],[0.95560,0.78005,0.22836],[0.96049,0.77181,0.22811],[0.96507,0.76352,0.22754],[0.96931,0.75519,0.22663],[0.97323,0.74682,0.22536],[0.97679,0.73842,0.22369],[0.98000,0.73000,0.22161],[0.98289,0.72140,0.21918],[0.98549,0.71250,0.21650],[0.98781,0.70330,0.21358],[0.98986,0.69382,0.21043],[0.99163,0.68408,0.20706],[0.99314,0.67408,0.20348],[0.99438,0.66386,0.19971],[0.99535,0.65341,0.19577],[0.99607,0.64277,0.19165],[0.99654,0.63193,0.18738],[0.99675,0.62093,0.18297],[0.99672,0.60977,0.17842],[0.99644,0.59846,0.17376],[0.99593,0.58703,0.16899],[0.99517,0.57549,0.16412],[0.99419,0.56386,0.15918],[0.99297,0.55214,0.15417],[0.99153,0.54036,0.14910],[0.98987,0.52854,0.14398],[0.98799,0.51667,0.13883],[0.98590,0.50479,0.13367],[0.98360,0.49291,0.12849],[0.98108,0.48104,0.12332],[0.97837,0.46920,0.11817],[0.97545,0.45740,0.11305],[0.97234,0.44565,0.10797],[0.96904,0.43399,0.10294],[0.96555,0.42241,0.09798],[0.96187,0.41093,0.09310],[0.95801,0.39958,0.08831],[0.95398,0.38836,0.08362],[0.94977,0.37729,0.07905],[0.94538,0.36638,0.07461],[0.94084,0.35566,0.07031],[0.93612,0.34513,0.06616],[0.93125,0.33482,0.06218],[0.92623,0.32473,0.05837],[0.92105,0.31489,0.05475],[0.91572,0.30530,0.05134],[0.91024,0.29599,0.04814],[0.90463,0.28696,0.04516],[0.89888,0.27824,0.04243],[0.89298,0.26981,0.03993],[0.88691,0.26152,0.03753],[0.88066,0.25334,0.03521],[0.87422,0.24526,0.03297],[0.86760,0.23730,0.03082],[0.86079,0.22945,0.02875],[0.85380,0.22170,0.02677],[0.84662,0.21407,0.02487],[0.83926,0.20654,0.02305],[0.83172,0.19912,0.02131],[0.82399,0.19182,0.01966],[0.81608,0.18462,0.01809],[0.80799,0.17753,0.01660],[0.79971,0.17055,0.01520],[0.79125,0.16368,0.01387],[0.78260,0.15693,0.01264],[0.77377,0.15028,0.01148],[0.76476,0.14374,0.01041],[0.75556,0.13731,0.00942],[0.74617,0.13098,0.00851],[0.73661,0.12477,0.00769],[0.72686,0.11867,0.00695],[0.71692,0.11268,0.00629],[0.70680,0.10680,0.00571],[0.69650,0.10102,0.00522],[0.68602,0.09536,0.00481],[0.67535,0.08980,0.00449],[0.66449,0.08436,0.00424],[0.65345,0.07902,0.00408],[0.64223,0.07380,0.00401],[0.63082,0.06868,0.00401],[0.61923,0.06367,0.00410],[0.60746,0.05878,0.00427],[0.59550,0.05399,0.00453],[0.58336,0.04931,0.00486],[0.57103,0.04474,0.00529],[0.55852,0.04028,0.00579],[0.54583,0.03593,0.00638],[0.53295,0.03169,0.00705],[0.51989,0.02756,0.00780],[0.50664,0.02354,0.00863],[0.49321,0.01963,0.00955],[0.47960,0.01583,0.01055]])\n",
    "\n",
    "# The look-up table contains 256 entries. Each entry is a floating point sRGB triplet.\n",
    "# To use it with matplotlib, pass cmap=ListedColormap(turbo_colormap_data) as an arg to imshow() \n",
    "# (don't forget \"from matplotlib.colors import ListedColormap\").\n",
    "# If you have a typical 8-bit greyscale image, you can use the 8-bit value to index into this LUT directly.\n",
    "# The floating point color values can be converted to 8-bit sRGB via multiplying by 255 and casting/flooring to \n",
    "# an integer. Saturation should not be required for IEEE-754 compliant arithmetic.\n",
    "# If you have a floating point value in the range [0,1], you can use interpolate() to linearly interpolate \n",
    "# between the entries.\n",
    "# If you have 16-bit or 32-bit integer values, convert them to floating point values on the [0,1] range \n",
    "# and then use interpolate(). Doing the interpolation in floating point will reduce banding.\n",
    "# If some of your values may lie outside the [0,1] range, use interpolate_or_clip() to highlight them.\n",
    "\n",
    "def interpolate(colormap, x):\n",
    "    x = np.minimum(0.0, np.minimum(1.0, x))\n",
    "    a = x*255.0\n",
    "    a = a.astype(int)\n",
    "    b = np.minimum(255, a + 1)\n",
    "    b = np.floor(b).astype(int)\n",
    "    f = x*255.0 - a\n",
    "    f = f.astype(float)\n",
    "    return np.array([colormap[a][0] + (colormap[b][0] - colormap[a][0]) * f,\n",
    "            colormap[a][1] + (colormap[b][1] - colormap[a][1]) * f,\n",
    "            colormap[a][2] + (colormap[b][2] - colormap[a][2]) * f])\n",
    "\n",
    "def interpolate_or_clip(colormap, x):\n",
    "    if   x < 0.0: return [0.0, 0.0, 0.0]\n",
    "    elif x > 1.0: return [1.0, 1.0, 1.0]\n",
    "    else: return interpolate(colormap, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cluster the pixel features using yet another fully connected auto encoder with shared weights (1x1 convs). The loss function is L=|p-D(E(p))|^2. The code is analogous to a ICA (independent component analysis). Minimizing the loss function is equivalent to maximizing the mutual information between the code and the original pixel, which is just the ICA analysis. \n",
    "\n",
    "f:features\n",
    "\n",
    "c:code\n",
    "\n",
    "I(f;c)=H(f) - H(f|c)\n",
    "\n",
    "f = D(c) + n\n",
    "\n",
    "P(f|c) = P(n) = P_D(p-D(c)) = P(L)\n",
    "\n",
    "L = |p-D(c)|^2\n",
    "\n",
    "argmax_D I = argmin_D L\n",
    "\n",
    "The autoencoder can be linear (ICA) or nonlinear (deep).\n",
    "The code can be visualised using colormaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_seg_model(images, model):\n",
    "    features = model(images)\n",
    "    return features\n",
    "\n",
    "@tf.function\n",
    "def imagesToPixels(image):\n",
    "    shape = tf.shape(image)\n",
    "    return tf.reshape(image, [shape[0], shape[1] * shape[2], shape[3]])\n",
    "\n",
    "\n",
    "def get_cmap(colormap_data):\n",
    "        def cmap(x):\n",
    "            return interpolate(colormap_data, x)\n",
    "        return cmap\n",
    "        \n",
    "def cmap_on_image(cmap_func):\n",
    "    return np.vectorize(cmap_func, signature=\"()->(3)\")\n",
    "\n",
    "\n",
    "def visualize_code(dst):\n",
    "    dst = dst.take(6)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    i = 0\n",
    "    for image in dst:\n",
    "        image = tf.image.convert_image_dtype(image, tf.uint8)\n",
    "        ax = plt.subplot(6, 1, i + 1)\n",
    "        plt.imshow(image.numpy().astype(\"uint8\"))\n",
    "        i+=1\n",
    "        plt.axis(\"off\")\n",
    "    return dst\n",
    "\n",
    "def split(image):\n",
    "    return tf.reshape(image, [-1,1,128])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = \"pad_to_size_1\"\n",
    "seg_model = keras.Model(inputs=model.input,\n",
    "                        outputs=model.get_layer(layer_name).output)\n",
    "# seg_model.trainable = False # freezing not needed since the models are not actually connected\n",
    "\n",
    "features_img = tf.keras.Input(shape=[128, 128, 128], dtype='float32', name=\"input_org\")\n",
    "code = keras.layers.Conv2D(filters=256, kernel_size=(1,1), data_format='channels_last')(features_img)\n",
    "code = keras.layers.ReLU()(code)\n",
    "code = keras.layers.Conv2D(filters=64, kernel_size=(1,1), data_format='channels_last')(features_img)\n",
    "code = keras.layers.ReLU()(code)\n",
    "code = keras.layers.Conv2D(filters=16, kernel_size=(1,1), data_format='channels_last')(code)\n",
    "code = keras.layers.ReLU(name=\"encoder_out\")(code)\n",
    "decoder = keras.layers.Conv2D(filters=64, kernel_size=(1,1), data_format='channels_last')(code)\n",
    "code = keras.layers.ReLU()(decoder)\n",
    "decoder = keras.layers.Conv2D(filters=256, kernel_size=(1,1), data_format='channels_last')(decoder)\n",
    "code = keras.layers.ReLU()(decoder)\n",
    "decoder = keras.layers.Conv2D(filters=128, kernel_size=(1,1), \n",
    "                              data_format='channels_last', name=\"decoder_out\")(decoder)\n",
    "\n",
    "autoencoder_model = keras.Model(inputs=features_img, outputs=decoder)\n",
    "\n",
    "\n",
    "get_gt = lambda image, gt, w: gt\n",
    "\n",
    "print(\"building clustering training pipline\")\n",
    "extract_seg_features = lambda images: do_seg_model(images, seg_model)\n",
    "\n",
    "# training data pipeline\n",
    "# shuffle and batch\n",
    "images_per_batch = 10\n",
    "train_dataset_seg_batched = train_dataset_seg.batch(images_per_batch, drop_remainder=True)\n",
    "val_dataset_seg_batched   = val_dataset_seg.batch(images_per_batch, drop_remainder=True)\n",
    "\n",
    "train_dataset_seg_pixels = train_dataset_seg_batched.map(get_gt, num_parallel_calls=3)\n",
    "val_dataset_seg_batched  = val_dataset_seg_batched.map(get_gt, num_parallel_calls=3)\n",
    "train_dataset_seg_pixels = train_dataset_seg_pixels.map(extract_seg_features, num_parallel_calls=3)\n",
    "val_dataset_seg_batched  = val_dataset_seg_batched.map(extract_seg_features, num_parallel_calls=3)\n",
    "\n",
    "print(\"pca_train_dataset:\",train_dataset_seg_pixels)\n",
    "# process 1000 pixels per batch instead of ~128 (memory effcient)\n",
    "autoencoder_train_dataset = train_dataset_seg_pixels.map(lambda x: (x,x)).prefetch(10)\n",
    "print(\"train_dataset_seg_pixels:\",train_dataset_seg_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_cp_file = f'{home}/session/autoencoder_deep_cp.h5'\n",
    "training = False\n",
    "if os.path.exists(ae_cp_file):\n",
    "    print(\"found checkpoint, loading\")\n",
    "    load_ae = True\n",
    "else:\n",
    "    load_ae = False\n",
    "if load_ae and not training:\n",
    "    autoencoder_model = tf.keras.models.load_model(ae_cp_file)\n",
    "    \n",
    "if training:\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001, epsilon=0.001) # default lr, use reduction strategy below\n",
    "    autoencoder_model.compile(optimizer=opt, loss=tf.keras.losses.MeanSquaredError(), metrics=['accuracy'])\n",
    "    if load_ae:\n",
    "        print(\"loading model weights for training\")\n",
    "        autoencoder_model.load_weights(filepath=ae_cp_file)\n",
    "    autoencoder_model.fit(autoencoder_train_dataset, \n",
    "                      epochs=3, validation_data=val_dataset, validation_steps=5, validation_freq=1,\n",
    "                        workers=3\n",
    "                        , callbacks=[tf.keras.callbacks.ModelCheckpoint(ae_cp_file, verbose=1, save_freq=500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = \"encoder_out\"\n",
    "encoder_model =  keras.Model(inputs=autoencoder_model.input,\n",
    "                        outputs=autoencoder_model.get_layer(layer_name).output)\n",
    "\n",
    "val_dataset_seg_code = val_dataset_seg_batched.map(encoder_model).unbatch()\n",
    "val_dataset_seg_code = val_dataset_seg_code.map(image_float_to_int)\n",
    "\n",
    "def visualize_seg(dst):\n",
    "    dst = dst.take(3)\n",
    "    for image in dst:\n",
    "        plt.figure(figsize=(30, 30))\n",
    "        for i in range(16):\n",
    "            ax = plt.subplot(4, 4, i + 1)\n",
    "            arr = image.numpy()\n",
    "            plt.imshow(arr[:,:,i])\n",
    "            i+=1\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_seg_code.apply(visualize_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for the most part the code corresponds to features which are spread across the whole picture. We don't see features that correspond to particular body parts like eyes etc. This makes sense since the results were quite blurry and seem to be somewhat of a naive extrapolation of the surroundings.\n",
    "\n",
    "This result is not due to the encoder since we can decode the pixels from it very precisely. In fact it may be that the features (and the code) correspond to the rgb values due to the UNET's skip connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt #2\n",
    "\n",
    "We will use a GAN architecture similar to infoGAN: https://arxiv.org/pdf/1606.03657.pdf\n",
    "\n",
    "However there are several changes relative to this work:\n",
    "\n",
    "1. As we want a segmentation map, we need to generate a code per pixel. Thus the architecture will be similar to what we tried above with two changes. We will not use skip connections between the encoder and decoder so we can clearly identify the code. Also we will use \"same\" padding and strided convolutions to mimic the downsampling while generating a code for each pixel.\n",
    "\n",
    "2. As was shown just above, maximizing the mutual information is the same as minimizing the distance between the decoded signal and the original signal. In infoGAN the mutual information I(c;G(c,z)) is maximized. Therefore we will minimize the distance L=|c-E(G(c,z))|^2. That is, we use the encoder part on the generated image to generate a code as similar as possible to the original one that was sampled while training the GAN. This part can be trained seperately from the generator (i.e. decoder) and the discriminator. Therefore the segmentor is actually the encoder which we will get \"for free\" with this method.\n",
    "\n",
    "3. I will use the WGAN method of https://arxiv.org/pdf/1701.07875.pdf. Additionaly we use the gradient penalty of https://arxiv.org/pdf/1704.00028.pdf\n",
    "\n",
    "To reduce training time the discriminator can actually be trained as an alternate \"head\" of the encoder. This is actually what was done in the infoGAN paper.\n",
    "\n",
    "In drawing samples from the latent space, we use a gaussian of unit variance. Since each pixel has N components in the code, we need each component to be a gaussian of variance 1/sqrt(N) (this way the total variance is 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(keras.layers.Layer):\n",
    "    '''\n",
    "    convolution block for unet with optional max pooling\n",
    "    '''\n",
    "    def __init__(self, filters, bname='', **kwargs):\n",
    "        self.filters = filters\n",
    "        self.bname = bname\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.conv1 = keras.layers.Conv2D(filters=self.filters, kernel_size=(3,3), strides=(1, 1),\n",
    "                            padding=\"same\", data_format='channels_last', kernel_initializer=\"glorot_uniform\",\n",
    "                            bias_initializer=\"zeros\", name=bname+\"_conv_1\")\n",
    "        self.conv2 = keras.layers.Conv2D(filters=self.filters, kernel_size=(3,3), strides=(1, 1),\n",
    "                            padding=\"same\", data_format='channels_last', kernel_initializer=\"glorot_uniform\",\n",
    "                            bias_initializer=\"zeros\", name=bname+\"_conv_2\")\n",
    "        self.conv3 = keras.layers.Conv2D(filters=self.filters, kernel_size=(3,3), strides=(2, 2),\n",
    "                            padding=\"same\", data_format='channels_last', kernel_initializer=\"glorot_uniform\",\n",
    "                            bias_initializer=\"zeros\", name=bname+\"_conv_3\")\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'filters': self.filters,\n",
    "            'bname': self.bname,\n",
    "            'downsample': self.downsample\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = keras.layers.LeakyReLU(alpha=0.1, name=self.bname+\"_relu_1\")(x)\n",
    "        x = self.conv2(x)\n",
    "        x = keras.layers.LeakyReLU(alpha=0.1, name=self.bname+\"_relu_2\")(x)\n",
    "        \n",
    "        x = tf.concat([x, inputs], axis=-1) # skip connection\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = keras.layers.LeakyReLU(alpha=0.1, name=self.bname+\"_relu_3\")(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeneratorBlock(keras.layers.Layer):\n",
    "    '''\n",
    "    convolution block for unet with optional max pooling\n",
    "    '''\n",
    "    def __init__(self, filters, bname='', **kwargs):\n",
    "        self.filters = filters\n",
    "        self.bname = bname\n",
    "        super(GeneratorBlock, self).__init__(**kwargs)\n",
    "        self.conv1 = keras.layers.Conv2DTranspose(filters=self.filters, kernel_size=(3,3), strides=2,\n",
    "                            padding=\"same\", data_format='channels_last', name=self.bname+\"_deconv\")\n",
    "        self.conv2 = keras.layers.Conv2D(filters=self.filters, kernel_size=(3,3), strides=1,\n",
    "                            padding=\"same\", data_format='channels_last', name=self.bname+\"_conv_1\")\n",
    "        self.conv3 = keras.layers.Conv2D(filters=self.filters, kernel_size=(3,3), strides=1,\n",
    "                            padding=\"same\", data_format='channels_last', name=self.bname+\"_conv_2\")\n",
    "        \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'filters': self.filters,\n",
    "            'bname': self.bname\n",
    "        })\n",
    "        return config\n",
    "        \n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x1 = keras.layers.ReLU(name=self.bname+\"_relu_1\")(x)\n",
    "        x2 = self.conv2(x1)\n",
    "        x2 = keras.layers.ReLU(name=self.bname+\"_relu_2\")(x2)\n",
    "        x3 = self.conv3(x2)\n",
    "        x3 = keras.layers.ReLU(name=self.bname+\"_relu_3\")(x3)\n",
    "        x4 = tf.math.add(x, x3) # skip connection\n",
    "        \n",
    "        return x4\n",
    "    \n",
    "\n",
    "def Encoder(code_features, pixel_features, decoder_filters_list):\n",
    "    x = tf.keras.Input(shape=[128,128, 3], dtype='float32', name=\"code\")\n",
    "    y = x\n",
    "    for i, f in enumerate(reversed(decoder_filters_list)):\n",
    "        bname = \"encoder_layer_\"+str(i)\n",
    "        y = EncoderBlock(f, bname=bname)(y)\n",
    "    bname = \"encoder_coded_pixels\"\n",
    "    y = EncoderBlock(pixel_features, bname=bname)(y)\n",
    "    y = keras.layers.LeakyReLU(alpha=0.1, name=bname+\"_relu\")(y)\n",
    "    y = tf.keras.layers.Conv2D(filters=code_features, kernel_size=(3, 3), strides=(1, 1), \n",
    "                               padding=\"same\", data_format='channels_last', name=\"encoder_out\")(y)\n",
    "    code_shape = y.shape\n",
    "    return keras.Model(inputs=x, outputs=y), code_shape\n",
    "\n",
    "def Generator(code_shape, noise_features, pixel_features, decoder_filters_list):\n",
    "    x = tf.keras.Input(shape=[code_shape[1], code_shape[2], code_shape[3] + noise_features], \n",
    "                       dtype='float32', name=\"code\")\n",
    "    y = x\n",
    "    for i, f in enumerate(decoder_filters_list):\n",
    "        bname = \"generator_layer_\"+str(i)\n",
    "        y = GeneratorBlock(f, bname=bname)(y)\n",
    "    \n",
    "    name = \"generator_coded_pixels\"\n",
    "    y = keras.layers.Conv2D(filters=pixel_features, kernel_size=(3, 3), strides=(1, 1),\n",
    "                            padding=\"same\", data_format='channels_last', name=name)(y)\n",
    "    y = keras.layers.ReLU(name=bname+\"_relu\")(y)\n",
    "    \n",
    "    bname = \"generator_out\"\n",
    "    y = keras.layers.Conv2D(filters=3, kernel_size=(3,3), strides=1,\n",
    "                            padding=\"same\", data_format='channels_last', name=bname+\"_deconv\")(y)\n",
    "    return keras.Model(inputs=x, outputs=y)\n",
    "\n",
    "def Critic(decoder_filters_list):\n",
    "    x = tf.keras.Input(shape=[128,128, 3], dtype='float32', name=\"code\")\n",
    "    y = x\n",
    "    for i, f in enumerate(reversed(decoder_filters_list)):\n",
    "        bname = \"critic_encoder_layer_\"+str(i)\n",
    "        y = EncoderBlock(f, bname=bname)(y)\n",
    "    bname = \"critic_head_layer\"\n",
    "    y = tf.keras.layers.Flatten()(y)\n",
    "    y = tf.keras.layers.Dense(units=1, name=bname)(y)\n",
    "    return keras.Model(inputs=x, outputs=y)\n",
    "\n",
    "\n",
    "class ShowGeneratorCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    def print_pics():\n",
    "        batch_size = tf.constant(9)\n",
    "        code_shape = self.model.code_shape\n",
    "        code_features = self.model.code_features\n",
    "        noise_features = self.model.noise_features\n",
    "        \n",
    "        latent_shape = (batch_size, code_shape[1], code_shape[2], \n",
    "                             code_features + noise_features)\n",
    "        random_latent_vectors = tf.random.normal(shape=latent_shape)\n",
    "        \n",
    "        random_pics = self.model.generator(random_latent_vectors)\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        for i in range(9):\n",
    "            image = random_pics[i]\n",
    "            image = tf.image.convert_image_dtype(image, tf.uint8)\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(image.numpy().astype(\"uint8\"))\n",
    "            plt.axis(\"off\")\n",
    "            \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(\"ShowGeneratorCallback: on_train_batch_end, batch=\", batch)\n",
    "        if batch % 100 != 1 :\n",
    "            return\n",
    "        \n",
    "        print(\"ShowGeneratorCallback: on_train_batch_end, drawing pics\")\n",
    "        print_pics()\n",
    "\n",
    "\n",
    "class InfoWGAN(keras.Model):\n",
    "    def __init__(self, code_features, noise_features, pixel_features, decoder_filters_list):\n",
    "        super(InfoWGAN, self).__init__()\n",
    "        self.coder, code_shape = Encoder(code_features, pixel_features, decoder_filters_list)\n",
    "        self.critic = Critic(decoder_filters_list)\n",
    "        self.generator = Generator(code_shape, noise_features, pixel_features, decoder_filters_list)\n",
    "        \n",
    "        self.code_shape = code_shape\n",
    "        self.noise_features = noise_features\n",
    "        self.code_features = code_features\n",
    "        self.pixel_features = pixel_features\n",
    "        \n",
    "        \n",
    "        self.real_label = tf.constant(1, dtype=tf.float32)\n",
    "        self.fake_label = tf.constant(-1, dtype=tf.float32)\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, q_optimizer, gradLAMBDA, infoLAMBDA):\n",
    "        super(InfoWGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.q_optimizer = q_optimizer\n",
    "        self.gradLAMBDA = gradLAMBDA\n",
    "        self.infoLAMBDA = infoLAMBDA\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            real_images = data[0]\n",
    "        else:\n",
    "            real_images = data\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        latent_shape = (batch_size, self.code_shape[1], self.code_shape[2], \n",
    "                             self.code_features + self.noise_features)\n",
    "        random_latent_vectors = tf.random.normal(shape=latent_shape)\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "        \n",
    "        # generate random \"intermidiate\" images interpolating the generated and real images for gradient penalty\n",
    "        eps = tf.random.uniform(shape=[batch_size, 1, 1, 1])\n",
    "        interp_images = tf.math.multiply(eps, real_images) + tf.math.multiply((1-eps), generated_images)\n",
    "\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, \n",
    "                                     real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat([self.fake_label * tf.ones((batch_size, 1, 1, 1)), \n",
    "                            self.real_label * tf.ones((batch_size, 1, 1, 1))], axis=0)\n",
    "\n",
    "        criticism = self.critic(combined_images)\n",
    "        wgan_loss = tf.reduce_mean(labels * criticism)\n",
    "        # get grad_x(critic(interpolated_images))\n",
    "        with tf.GradientTape() as xtape:\n",
    "            xtape.watch(interp_images)\n",
    "            interp_criticism = self.critic(interp_images)\n",
    "        critic_x_grad = xtape.gradient(interp_criticism, interp_images)\n",
    "        penalty_loss = tf.reduce_mean(tf.square(tf.norm(critic_x_grad, axis=-1) - 1))\n",
    "        d_loss = wgan_loss + self.gradLAMBDA * penalty_loss + self.critic.losses\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=latent_shape)\n",
    "        random_code = random_latent_vectors[:,:,:,:self.code_features]\n",
    "\n",
    "        # Assemble labels that say \"all real images\" \n",
    "        # This makes the generator want to create real images (match the label) since \n",
    "        # we do not include an additional minus in the loss\n",
    "        misleading_labels = self.real_label * tf.ones((batch_size, 1, 1, 1))\n",
    "\n",
    "        # Train the generator and encoder(note that we should *not* update the weights\n",
    "        # of the critic or encoder)!\n",
    "        fakeImages = self.generator(random_latent_vectors)\n",
    "        criticism = self.critic(fakeImages)\n",
    "        code_pred = self.coder(fakeImages)\n",
    "        g_loss = tf.reduce_mean(misleading_labels * criticism)\n",
    "        info_loss = tf.reduce_mean(tf.math.squared_difference(code_pred, random_code))\n",
    "        \n",
    "        return {\"critic_loss\": wgan_loss, \"generator_loss\": g_loss, \"info_loss\": info_loss, \n",
    "                \"gradient_penalty_loss\": penalty_loss}\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            real_images = data[0]\n",
    "        else:\n",
    "            real_images = data\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        latent_shape = (batch_size, self.code_shape[1], self.code_shape[2], \n",
    "                        self.code_features + self.noise_features)\n",
    "        random_latent_vectors = tf.random.normal(shape=latent_shape)\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "        \n",
    "        # generate random \"intermidiate\" images interpolating the generated and real images for gradient penalty\n",
    "        eps = tf.random.uniform(shape=[batch_size, 1, 1, 1])\n",
    "        interp_images = tf.math.multiply(eps, real_images) + tf.math.multiply((1-eps), generated_images)\n",
    "\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, \n",
    "                                     real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat([self.fake_label * tf.ones((batch_size, 1, 1, 1)), \n",
    "                            self.real_label * -tf.ones((batch_size, 1, 1, 1))], axis=0)\n",
    "\n",
    "        # Train the discriminator to optimality\n",
    "        for step in range(5):\n",
    "            with tf.GradientTape() as tape:\n",
    "                criticism = self.critic(combined_images)\n",
    "                wgan_loss = tf.reduce_mean(tf.math.multiply(labels, criticism))\n",
    "                # get grad_x(critic(interpolated_images))\n",
    "                with tf.GradientTape() as xtape:\n",
    "                    xtape.watch(interp_images)\n",
    "                    interp_criticism = self.critic(interp_images)\n",
    "                critic_x_grad = xtape.gradient(interp_criticism, interp_images)\n",
    "                penalty_loss = tf.reduce_mean(tf.square(tf.norm(critic_x_grad, axis=-1) - 1))\n",
    "                d_loss = wgan_loss + self.gradLAMBDA * penalty_loss + self.critic.losses\n",
    "                \n",
    "            critic_grads = tape.gradient(d_loss, self.critic.trainable_weights)\n",
    "            self.d_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_weights))\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=latent_shape)\n",
    "        random_code = random_latent_vectors[:,:,:,:self.code_features]\n",
    "\n",
    "        # Assemble labels that say \"all real images\" \n",
    "        # This makes the generator want to create real images (match the label) since \n",
    "        # we do not include an additional minus in the loss\n",
    "        misleading_labels = self.real_label * tf.ones((batch_size, 1, 1, 1))\n",
    "\n",
    "        # Train the generator and encoder(note that we should *not* update the weights\n",
    "        # of the critic or encoder)!\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            fakeImages = self.generator(random_latent_vectors)\n",
    "            criticism = self.critic(fakeImages)\n",
    "            code_pred = self.coder(fakeImages)\n",
    "            g_loss = tf.reduce_mean(misleading_labels * criticism)\n",
    "            info_loss = tf.reduce_mean(tf.math.squared_difference(code_pred, random_code))\n",
    "            total_g_loss = g_loss + self.infoLAMBDA * info_loss + self.generator.losses + self.coder.losses\n",
    "            \n",
    "        g_grads = tape.gradient(total_g_loss, self.generator.trainable_weights)\n",
    "        q_grads = tape.gradient(total_g_loss, self.coder.trainable_weights)\n",
    "        del tape\n",
    "        self.g_optimizer.apply_gradients(zip(g_grads, self.generator.trainable_weights))\n",
    "        self.q_optimizer.apply_gradients(zip(q_grads, self.coder.trainable_weights))\n",
    "        \n",
    "        return {\"critic_loss\": wgan_loss, \"generator_loss\": g_loss, \"info_loss\": info_loss, \n",
    "                \"gradient_penalty_loss\": penalty_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### prepare data pipiline\n",
    "\n",
    "\n",
    "station = \"home\"\n",
    "# station = \"aws\"\n",
    "\n",
    "if station==\"aws\":\n",
    "    home = os.getenv(\"HOME\")\n",
    "elif station==\"home\":\n",
    "    home = \"/home/lior/PycharmProjects/facesTasks\"\n",
    "    \n",
    "print(\"current working directory:\", home)\n",
    "\n",
    "images_per_batch = 32\n",
    "dataset = tf.data.Dataset.list_files(f'{home}/images/*.jpg')\n",
    "# train/test split\n",
    "image_count = tf.cast(dataset.cardinality(), tf.float32)\n",
    "train_perc = tf.constant(0.8)\n",
    "train_dataset = dataset.take(tf.cast(tf.math.round(image_count * train_perc), tf.int64))\n",
    "val_dataset = dataset.skip(tf.cast(tf.math.round(image_count * train_perc), tf.int64))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000)\n",
    "val_dataset = val_dataset.shuffle(buffer_size=1000)\n",
    "\n",
    "# read the images\n",
    "train_dataset = train_dataset.map(load_image, num_parallel_calls=3).cache()\n",
    "train_dataset = train_dataset.map(partial(tf.image.convert_image_dtype, dtype=tf.float32), num_parallel_calls=3)\n",
    "val_dataset = val_dataset.map(load_image, num_parallel_calls=3).cache()\n",
    "val_dataset = val_dataset.map(partial(tf.image.convert_image_dtype, dtype=tf.float32), num_parallel_calls=3)\n",
    "\n",
    "# prepare augmented images. dataset of (augmented image, image), block_length augmentions for each\n",
    "train_dataset = train_dataset.map(partial(random_crop, size=128), num_parallel_calls=3)\n",
    "val_dataset = val_dataset.map(partial(random_crop, size=128), num_parallel_calls=3)\n",
    "\n",
    "# shuffle and batch\n",
    "train_dataset = train_dataset.batch(images_per_batch, drop_remainder=True)\n",
    "val_dataset   = val_dataset.batch(images_per_batch, drop_remainder=True)\n",
    "# prefetch\n",
    "train_dataset = train_dataset.prefetch(2)\n",
    "val_dataset = val_dataset.prefetch(2)\n",
    "\n",
    "print(\"done dataset preparations\")\n",
    "print(train_dataset)\n",
    "\n",
    "decoder_filters_list = [128, 128, 64, 64,  32, 32, 16]\n",
    "code_features = 16\n",
    "noise_features = 4\n",
    "pixel_features = 16\n",
    "infoLAMBDA = 0.001\n",
    "gradLAMBDA = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_cp_file = f'{home}/session/infoWGAN_cp.h5'\n",
    "training = True\n",
    "if os.path.exists(gan_cp_file):\n",
    "    print(\"found checkpoint, loading\")\n",
    "    load = True\n",
    "else:\n",
    "    load = False\n",
    "if load and not training:\n",
    "    gan_trainer = tf.keras.models.load_model(gan_cp_file)\n",
    "else:\n",
    "    gan_trainer = InfoWGAN(code_features, noise_features, pixel_features, decoder_filters_list)\n",
    "    \n",
    "if training:\n",
    "    d_opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0, beta_2=0.9, epsilon=1e-05, name='Adam')\n",
    "    g_opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0, beta_2=0.9, epsilon=1e-05, name='Adam')\n",
    "    q_opt = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0, beta_2=0.9, epsilon=1e-05, name='Adam')\n",
    "    gan_trainer.compile(d_opt, g_opt, q_opt, gradLAMBDA, infoLAMBDA)\n",
    "    if load:\n",
    "        print(\"loading model weights for training\")\n",
    "        gan_trainer.load_weights(filepath=ae_cp_file)\n",
    "    \n",
    "    gan_trainer.generator.summary()\n",
    "    gan_trainer.critic.summary()\n",
    "    gan_trainer.coder.summary()\n",
    "    gan_trainer.fit(train_dataset, epochs=3, validation_data=val_dataset, validation_steps=5, validation_freq=1,\n",
    "                workers=3, callbacks=[tf.keras.callbacks.ModelCheckpoint(gan_cp_file, verbose=1, save_freq=500),\n",
    "                                     ShowGeneratorCallback()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearningAWS",
   "language": "python",
   "name": "dlaws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
